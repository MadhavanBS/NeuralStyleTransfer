{"metadata":{"colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8537232,"sourceType":"datasetVersion","datasetId":5083656}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport copy\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\nimport scipy.optimize as sopt\nvgg19 = tf.keras.applications.VGG19(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(400,400,3)\n)\n","metadata":{"id":"MsHEGMddoD_E","outputId":"42d0e5e1-e078-4df1-c1fd-3c627043458c","execution":{"iopub.status.busy":"2024-05-27T09:15:19.331409Z","iopub.execute_input":"2024-05-27T09:15:19.331893Z","iopub.status.idle":"2024-05-27T09:15:31.516347Z","shell.execute_reply.started":"2024-05-27T09:15:19.331866Z","shell.execute_reply":"2024-05-27T09:15:31.515210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport scipy.optimize as sopt\nif not os.path.exists('/kaggle/working/nst/'):\n    os.mkdir('/kaggle/working/nst/', mode=0o666)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T09:15:31.518136Z","iopub.execute_input":"2024-05-27T09:15:31.518656Z","iopub.status.idle":"2024-05-27T09:15:31.523538Z","shell.execute_reply.started":"2024-05-27T09:15:31.518628Z","shell.execute_reply":"2024-05-27T09:15:31.522573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg19.save('/kaggle/working/nst/vgg.keras')","metadata":{"id":"3MmEuoIfr6bG","outputId":"9493d042-6427-4799-bb89-14da25586005","execution":{"iopub.status.busy":"2024-05-27T09:15:31.524836Z","iopub.execute_input":"2024-05-27T09:15:31.525160Z","iopub.status.idle":"2024-05-27T09:15:31.844218Z","shell.execute_reply.started":"2024-05-27T09:15:31.525132Z","shell.execute_reply":"2024-05-27T09:15:31.843211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import AveragePooling2D\ncustom_objects = {'MaxPooling2D': AveragePooling2D}\nmvgg19 = tf.keras.models.load_model('/kaggle/working/nst/vgg.keras', custom_objects = custom_objects)","metadata":{"id":"coVzWOV7y24E","outputId":"e51278ca-103e-4247-96a5-cc602455a40d","execution":{"iopub.status.busy":"2024-05-27T09:15:31.846351Z","iopub.execute_input":"2024-05-27T09:15:31.846647Z","iopub.status.idle":"2024-05-27T09:15:32.973282Z","shell.execute_reply.started":"2024-05-27T09:15:31.846624Z","shell.execute_reply":"2024-05-27T09:15:32.972522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"style_layers = [\"block1_conv1\", \"block2_conv1\", \"block3_conv1\", \"block4_conv1\", \"block5_conv1\"]\ncontent_layers = [\"block5_conv2\"]","metadata":{"id":"oOAAX8GNscut","execution":{"iopub.status.busy":"2024-05-27T09:15:32.974420Z","iopub.execute_input":"2024-05-27T09:15:32.974764Z","iopub.status.idle":"2024-05-27T09:15:32.979784Z","shell.execute_reply.started":"2024-05-27T09:15:32.974732Z","shell.execute_reply":"2024-05-27T09:15:32.978853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vgg_layers2model(layer_names):\n  outputs = [mvgg19.get_layer(layer_name).output for layer_name in layer_names]\n  model = tf.keras.Model(inputs=mvgg19.input,outputs=outputs)\n  return model","metadata":{"id":"cr7e_uUytqTg","execution":{"iopub.status.busy":"2024-05-27T09:15:32.981006Z","iopub.execute_input":"2024-05-27T09:15:32.981322Z","iopub.status.idle":"2024-05-27T09:15:32.987851Z","shell.execute_reply.started":"2024-05-27T09:15:32.981293Z","shell.execute_reply":"2024-05-27T09:15:32.987138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nst_model = vgg_layers2model(style_layers+content_layers)","metadata":{"id":"A3fKnE63zwW7","execution":{"iopub.status.busy":"2024-05-27T09:15:32.988897Z","iopub.execute_input":"2024-05-27T09:15:32.989453Z","iopub.status.idle":"2024-05-27T09:15:33.000894Z","shell.execute_reply.started":"2024-05-27T09:15:32.989422Z","shell.execute_reply":"2024-05-27T09:15:33.000086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nst_model.trainable = False","metadata":{"id":"0BSWGQumZTgh","execution":{"iopub.status.busy":"2024-05-27T09:15:33.001855Z","iopub.execute_input":"2024-05-27T09:15:33.002158Z","iopub.status.idle":"2024-05-27T09:15:33.009577Z","shell.execute_reply.started":"2024-05-27T09:15:33.002129Z","shell.execute_reply":"2024-05-27T09:15:33.008656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nst_model.summary()","metadata":{"id":"CfFuN6Ka0C-P","outputId":"368578a7-d1c9-4652-ebe9-badbdf64c9da","execution":{"iopub.status.busy":"2024-05-27T09:15:33.010667Z","iopub.execute_input":"2024-05-27T09:15:33.011191Z","iopub.status.idle":"2024-05-27T09:15:33.045696Z","shell.execute_reply.started":"2024-05-27T09:15:33.011160Z","shell.execute_reply":"2024-05-27T09:15:33.044933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nst_model.outputs","metadata":{"id":"kRwCTLEhVXIf","outputId":"a28ee8dd-dd41-47cf-f9cf-619ddbfee75b","execution":{"iopub.status.busy":"2024-05-27T09:15:33.048931Z","iopub.execute_input":"2024-05-27T09:15:33.049183Z","iopub.status.idle":"2024-05-27T09:15:33.054702Z","shell.execute_reply.started":"2024-05-27T09:15:33.049162Z","shell.execute_reply":"2024-05-27T09:15:33.053755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\n\nimg_size=400\nstyle_image =  np.array(Image.open(\"/kaggle/input/imagesfornst/style37.jpg\").resize((img_size, img_size)))\n#style_image = preprocess_input(style_image)\nstyle_image = tf.constant(np.reshape(style_image, ((1,) + style_image.shape)))\n\nprint(style_image.shape)\nimshow(style_image[0])\nplt.show()","metadata":{"id":"yaZDdAbj0Esj","outputId":"54894359-80ab-4fd7-c1ff-7774852fab5d","execution":{"iopub.status.busy":"2024-05-27T09:15:33.055722Z","iopub.execute_input":"2024-05-27T09:15:33.056033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\ncontent_image_np = np.array(Image.open(\"/kaggle/input/imagesfornst/content21.jpg\").resize((img_size, img_size)))\n#content_image = preprocess_input(content_image_np)\ncontent_image = copy.deepcopy(content_image_np)\ncontent_image = tf.constant(np.reshape(content_image, ((1,) + content_image.shape)))\nprint(content_image.shape)\nimshow(content_image[0])\nplt.show()","metadata":{"id":"Ow7BvsAnOu-z","outputId":"4a9b4fc9-1a53-436e-8c32-1e905e612f1f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def content_cost(orig_img_features, gen_img_features):\n#     _, layer_height, layer_width, layer_channels = gen_img_features.shape\n#     orig_img_features_unrolled = tf.reshape(orig_img_features, [layer_height * layer_width, layer_channels])\n#     gen_img_features_unrolled = tf.reshape(gen_img_features, [layer_height * layer_width, layer_channels])\n#     #content_cost = tf.reduce_sum(tf.square(orig_img_features_unrolled - gen_img_features_unrolled))/((4*layer_height*layer_width)**0.5)\n#     content_cost = tf.reduce_sum(tf.square(orig_img_features_unrolled - gen_img_features_unrolled))/2   \n#     return content_cost","metadata":{"id":"ZETJBYQkPLOE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def deprocess_img(processes_img):\n#   x = processes_img.numpy()\n#   if len(x.shape) == 4:\n#     x = np.squeeze(x, 0)\n#   x[:, :, 0] += 103.939\n#   x[:, :, 1] += 116.779\n#   x[:, :, 2] += 123.68\n#   x = x[:, :, ::-1] # converting BGR to RGB channel\n#   x = np.clip(x, 0, 255).astype('uint8')\n#   return x\n\n# # imshow(deprocess_img(style_image))\n# # plt.show()\n","metadata":{"id":"ttXE6TAeGCnE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise_ratio = 0\nnoise_img = np.random.uniform(0, 255, (1, 400, 400, 3))\nwhite_noise_img = np.random.uniform(0, 255, (1, 400, 400, 3))\ncontent_image_2 = np.reshape(content_image_np, ((1,) + content_image_np.shape))\nnoise_img = (noise_ratio*noise_img)+((1-noise_ratio)*content_image_2)\n#noise_img = preprocess_input(noise_img)\nnoise_img = tf.Variable(noise_img, dtype='float32')\nwhite_noise_img = tf.Variable(white_noise_img, dtype='float32')","metadata":{"id":"9xOOtG4fN4Ty","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# noise image should be generated from the content image\n# use lbfgs instead of adam","metadata":{"id":"IG8e1uEDaDsw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def style_cost(style_features, gen_img_features):\n#     _, layer_height, layer_width, layer_channels = gen_img_features.shape\n#     style_features_unrolled = tf.reshape(style_features, [layer_channels, layer_height * layer_width])\n#     gen_img_features_unrolled = tf.reshape(gen_img_features, [layer_channels, layer_height * layer_width])\n\n#     gram_style_matrix = tf.linalg.matmul(style_features_unrolled, style_features_unrolled, transpose_b=True)\n#     gram_img_matrix = tf.linalg.matmul(gen_img_features_unrolled, gen_img_features_unrolled, transpose_b=True)\n\n#     return tf.reduce_sum(tf.square(tf.subtract(gram_style_matrix,gram_img_matrix)))/((2*layer_height*layer_width*layer_channels)**2)\n","metadata":{"id":"fw9diegPbwRd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def content_cost(p, x):\n  _, h, w, d = p.get_shape()\n  M = h*w\n  N = d\n  K = 0.5\n  loss = K * tf.reduce_sum(tf.pow((x - p), 2))\n  return loss\n\ndef style_cost(a, x):\n  _, h, w, d = a.get_shape()\n  M = h*w\n  N = d\n  A = gram_matrix(a, M, N)\n  G = gram_matrix(x, M, N)\n  loss = (1./(4 * N**2 * M**2)) * tf.reduce_sum(tf.pow((G - A), 2))\n  return loss\n\ndef gram_matrix(x, area, depth):\n  F = tf.reshape(x, (area, depth))\n  G = tf.matmul(tf.transpose(F), F)\n  return G","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def total_variation_loss(image):\n#   x_deltas = image[:, :, 1:, :] - image[:, :, :-1, :]\n#   y_deltas = image[:, 1:, :, :] - image[:, :-1, :, :]\n#   return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))\n  return tf.reduce_sum(tf.image.total_variation(image))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clip_0_1(noisy_image):\n    \"\"\"\n    Truncate all the pixels in the tensor to be between 0 and 255\n    \n    Arguments:\n    image -- Tensor\n    J_style -- style cost coded above\n\n    Returns:\n    Tensor\n    \"\"\"\n    return tf.clip_by_value(noisy_image, clip_value_min=0, clip_value_max=255)\n\ndef tf2img(processes_img):\n  x = processes_img.numpy()\n  if len(x.shape) == 4:\n    x = np.squeeze(x, 0)\n  x = np.clip(x, 0, 255).astype('uint8')\n  return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orig_img_features = nst_model(content_image)[-1]\norig_style_features = nst_model(style_image)[:-1]\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\ntf.config.run_functions_eagerly(run_eagerly=True)\nalpha = 1\nbeta = 1\ngamma = 1\n@tf.function()\ndef train_step(noise_img):\n  with tf.GradientTape() as tape:\n    gen_img_features=nst_model(noise_img)\n    content_cost_value = content_cost(orig_img_features, gen_img_features[-1])\n    style_cost_value = 0\n    style_features_list=gen_img_features[:-1]\n    style_weights = [0.2,0.2,0.2,0.2,0.2]\n\n    for (i, gen_style_features) in zip(range(5), style_features_list):\n      style_cost_value+=(style_weights[i]*style_cost(orig_style_features[i], gen_style_features))\n    \n    variation_loss = total_variation_loss(noise_img)\n        \n    total_cost = alpha*content_cost_value + beta*style_cost_value + gamma*variation_loss\n    \n  grad = tape.gradient(total_cost, noise_img)\n  optimizer.apply_gradients([(grad, noise_img)]) \n  noise_img.assign(clip_0_1(noise_img))\n  return (content_cost_value), (style_cost_value), (variation_loss)","metadata":{"id":"EjVsymPShLvx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_cost_value, style_cost_value, variation_loss = train_step(white_noise_img)\ntotal_cost = alpha*content_cost_value + beta*style_cost_value + gamma*variation_loss\naverage_cost = total_cost/3\nalpha = average_cost/content_cost_value\nbeta = 100*average_cost/style_cost_value\ngamma = 0.1*average_cost/variation_loss\nprint(f'alpha is {alpha}\\nbeta is {beta}\\ngamma is {gamma}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = tf2img(noise_img)\nimshow(img)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orig_img_features = nst_model(content_image)[-1]\norig_style_features = nst_model(style_image)[:-1]\ntf.config.run_functions_eagerly(run_eagerly=True)\n\n@tf.function\ndef compute_loss_and_grads(noise_img):\n    with tf.GradientTape() as tape:\n        tape.watch(noise_img)\n        \n        gen_img_features = nst_model(noise_img)\n        content_cost_value = content_cost(orig_img_features, gen_img_features[-1])\n        \n        style_cost_value = 0\n        style_features_list = gen_img_features[:-1]\n        style_weights = [0.2, 0.2, 0.2, 0.2, 0.2]\n        \n        for i, gen_style_features in zip(range(5), style_features_list):\n            style_cost_value += (style_weights[i] * style_cost(orig_style_features[i], gen_style_features))\n        \n        variation_loss = total_variation_loss(noise_img)\n        \n        total_cost = alpha * content_cost_value + beta * style_cost_value + gamma * variation_loss\n        print(f\"content_cost: {alpha*content_cost_value}, style_cost is {beta*style_cost_value}, variation_loss is {gamma*variation_loss}\")\n    grads = tape.gradient(total_cost, noise_img)\n    return total_cost, grads\n\n# Wrapper function for SciPy optimizer\ndef func(x):\n    noise_img = tf.Variable(x.reshape(content_image.shape), dtype=tf.float32)\n    #noise_img = clip_0_1(noise_img)\n    loss, grads = compute_loss_and_grads(noise_img)\n    return loss.numpy().astype(np.float64), grads.numpy().flatten().astype(np.float64)\n\n\n# Run the L-BFGS-B optimization\nresult = sopt.minimize(fun=func, x0=noise_img.numpy().flatten(), jac=True, method='L-BFGS-B', options={'maxiter': 7000})\n\nnoise_img =  tf.Variable(result.x.reshape(content_image.shape), dtype=tf.float32)\nnoise_img = clip_0_1(noise_img)\nimg = tf2img(noise_img)\nimshow(img)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# content_cost_value, style_cost_value, variation_loss = train_step(noise_img)\n# print(f\"content cost: {content_cost_value}, style cost: {style_cost_value}, variation cost: {variation_loss}\")\n# img = tf2img(noise_img)\n# imshow(img)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# noise_img.numpy().min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gamma = 16000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(1001):\n#     try:\n#       content_cost_value, style_cost_value, variation_loss = train_step(noise_img)\n#     except Exception as ex:\n#       print(ex)\n#     if(i==20000):\n#          optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n#     if(i==40000):\n#          optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n# #     if i%100 == 0:\n# #           print(f\"content cost: {alpha*content_cost_value}, style cost: {beta*style_cost_value}\")\n#     if i%1000 == 0:\n#         img=tf2img(noise_img)\n#         imshow(img)\n#         plt.show()","metadata":{"id":"7uGJqyvkpaMG","outputId":"3097198c-d428-4efb-b2cd-98b16d930c5a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_to_save = Image.fromarray(img)\n# image_to_save.save('/kaggle/working/deprocessed_style_image.png')","metadata":{"id":"z4EMd3MIpfLX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#         img=tf2img(noise_img)\n#         imshow(img)\n#         plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise_img =  tf.Variable(result.x.reshape(content_image.shape), dtype=tf.float32)\nnoise_img = clip_0_1(noise_img)\narr = tf2img(noise_img)\nfrom PIL import Image\nim = Image.fromarray(arr)\nim.save(\"/kaggle/working/result3.png\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise_img =  tf.Variable(result.x.reshape(content_image.shape), dtype=tf.float32)\nnoise_img = clip_0_1(noise_img)\nimg = tf2img(noise_img)\nimshow(img)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # figsize controls the overall size of the figure\n\n# Display each image in a subplot\ncontent_image = tf2img(content_image)\nstyle_image = tf2img(style_image)\n\naxes[0].imshow(content_image)\naxes[0].axis('off')  # Hide the axis\naxes[0].set_title('Content') \n\naxes[1].imshow(style_image)\naxes[1].axis('off')  # Hide the axis\naxes[1].set_title('Style')\n\naxes[2].imshow(img)\naxes[2].axis('off')  # Hide the axis\naxes[2].set_title('Result Image')\n\nplt.savefig('/kaggle/working/result2.png', bbox_inches='tight', pad_inches=0.1)\n# Display the figure\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.savefig('/kaggle/working/result2.png', bbox_inches='tight', pad_inches=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}